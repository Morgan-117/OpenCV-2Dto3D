// Taken from:
// https://stackoverflow.com/questions/44104633/transforming-2d-image-coordinates-to-3d-world-coordinates-with-z-0
// https://github.com/IndrajeetDatta/Extrinsics
// http://answers.opencv.org/question/62779/image-coordinate-to-world-coordinate-opencv/
// https://stackoverflow.com/questions/12299870/computing-x-y-coordinate-3d-from-image-point
#include "opencv2/opencv.hpp"
#include <stdio.h>
#include <iostream>
#include <sstream>
#include <math.h>
#include <cstdlib>

cv::Mat cameraMatrix, distCoeffs, rotationVector, rotationMatrix, translationVector, invR_x_invM_x_uv1, invR_x_tvec, wcPoint;
std::vector <cv::Point2d> image_points;
std::vector <cv::Point3d> world_points;
double Z;

int main(int argc, char *argv[]) {
	// Intrinsics is already calculated
	cv::FileStorage fs1("intrinsics.yml", cv::FileStorage::READ);

	fs1["camera_matrix"]>>cameraMatrix;
	std::cerr << "Camera Matrix: " << cameraMatrix << std::endl << std::endl;

	fs1["distortion_coefficients"] >> distCoeffs;
	std::cerr << "Distortion Coefficients: " << distCoeffs << std::endl << std::endl;

	// Let's add 4 points image points...
	image_points.push_back(cv::Point2d(275, 204));
	image_points.push_back(cv::Point2d(331, 204));
	image_points.push_back(cv::Point2d(331, 308));
	image_points.push_back(cv::Point2d(257, 308));
	std::cerr << "Image Points: " << image_points << std::endl << std::endl;

	// ...corresponding to 4 world points...
	world_points.push_back(cv::Point3d(0.0, 0.0, 0.0));
	world_points.push_back(cv::Point3d(1.775, 0.0, 0.0));
	world_points.push_back(cv::Point3d(1.775, 4.620, 0.0));
	world_points.push_back(cv::Point3d(0.0, 4.620, 0.0));
	std::cerr << "World Points: " << world_points << std::endl << std::endl;

	// ... in order to get our matrix!
	solvePnP(world_points, image_points, cameraMatrix, distCoeffs, rotationVector, translationVector);
	Rodrigues(rotationVector, rotationMatrix);
	std::cerr << "Rotation Matrix: " << std::endl << rotationMatrix << std::endl << std::endl;

	// Example usage: transform the third point:
	// screen coordinates (331, 308) to world coordinates should produce (1.775, 4.620, Z).
	// we use the standard focal_distance=1 (generated by Mat::ones) and will define Z=0.
	cv::Mat screenCoordinates = cv::Mat::ones(3, 1, cv::DataType<double>::type);
	screenCoordinates.at<double>(0, 0) = 331;
	screenCoordinates.at<double>(1, 0) = 308;
	if(argc>2) {
		screenCoordinates.at<double>(0, 0) = atoi(argv[1]);
		screenCoordinates.at<double>(1, 0) = atoi(argv[2]);
	}
	screenCoordinates.at<double>(2, 0) = 1; // f=1
	std::cerr << "Camera Coordinates:" << screenCoordinates << std::endl << std::endl;

	// Hypothesis sol:
	Z=0;

	// s and point calculation, described here:
	// https://stackoverflow.com/questions/12299870/computing-x-y-coordinate-3d-from-image-point
	invR_x_invM_x_uv1=rotationMatrix.inv()*cameraMatrix.inv()*screenCoordinates;
	invR_x_tvec      =rotationMatrix.inv()*translationVector;
	wcPoint=(Z+invR_x_tvec.at<double>(2, 0))/invR_x_invM_x_uv1.at<double>(2, 0)*invR_x_invM_x_uv1-invR_x_tvec;
	cv::Point3f worldCoordinates(wcPoint.at<double>(0, 0), wcPoint.at<double>(1, 0), wcPoint.at<double>(2, 0));
	std::cerr << "World Coordinates" << worldCoordinates << std::endl << std::endl;
	std::cout 	<< screenCoordinates.at<double>(0, 0) << ","
			<< screenCoordinates.at<double>(1, 0) << ","
			<< worldCoordinates.x << ","
			<< worldCoordinates.y << std::endl;
	return 0;
}
